{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#The \"create_directories\" function ensures that directories for each category are created.\n",
    "def create_directories(base_dir, categories):\n",
    "    for category in categories:\n",
    "        dir_path = os.path.join(base_dir, category)\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "    print(f\"Directories created: {', '.join([os.path.join(base_dir, category) for category in categories])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting icrawler\n",
      "  Downloading icrawler-0.6.8-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from icrawler) (4.12.2)\n",
      "Collecting bs4 (from icrawler)\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Requirement already satisfied: lxml in c:\\programdata\\anaconda3\\lib\\site-packages (from icrawler) (4.9.3)\n",
      "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from icrawler) (10.2.0)\n",
      "Requirement already satisfied: pyyaml in c:\\programdata\\anaconda3\\lib\\site-packages (from icrawler) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from icrawler) (2.31.0)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from icrawler) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->icrawler) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->icrawler) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->icrawler) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->icrawler) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->icrawler) (2024.2.2)\n",
      "Downloading icrawler-0.6.8-py3-none-any.whl (34 kB)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Installing collected packages: bs4, icrawler\n",
      "Successfully installed bs4-0.0.2 icrawler-0.6.8\n"
     ]
    }
   ],
   "source": [
    "!pip install icrawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icrawler.builtin import GoogleImageCrawler\n",
    "\n",
    "def download_images(query, num_images, save_dir):\n",
    "    google_crawler = GoogleImageCrawler(storage={'root_dir': save_dir})\n",
    "    google_crawler.crawl(keyword=query, max_num=num_images)\n",
    "    print(f\"Downloaded images for {query} into {save_dir}\")\n",
    "# [source code] https://icrawler.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories\n",
    "base_dir = 'face_dataset'\n",
    "categories = ['nicolas_cage', 'others']\n",
    "create_directories(base_dir, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-30 15:44:54,889 - INFO - icrawler.crawler - start crawling...\n",
      "2024-05-30 15:44:54,890 - INFO - icrawler.crawler - starting 1 feeder threads...\n",
      "2024-05-30 15:44:54,891 - INFO - feeder - thread feeder-001 exit\n",
      "2024-05-30 15:44:54,893 - INFO - icrawler.crawler - starting 1 parser threads...\n",
      "2024-05-30 15:44:54,895 - INFO - icrawler.crawler - starting 1 downloader threads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories created: face_dataset\\nicolas_cage, face_dataset\\others\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-30 15:44:57,164 - INFO - parser - parsing result page https://www.google.com/search?q=Nicolas+Cage&ijn=0&start=0&tbs=&tbm=isch\n",
      "2024-05-30 15:44:59,235 - INFO - parser - no more page urls for thread parser-001 to parse\n",
      "2024-05-30 15:44:59,236 - INFO - parser - thread parser-001 exit\n",
      "2024-05-30 15:44:59,904 - INFO - downloader - no more download task for thread downloader-001\n",
      "2024-05-30 15:44:59,906 - INFO - downloader - thread downloader-001 exit\n",
      "2024-05-30 15:45:00,897 - INFO - icrawler.crawler - Crawling task done!\n",
      "2024-05-30 15:45:00,900 - INFO - icrawler.crawler - start crawling...\n",
      "2024-05-30 15:45:00,900 - INFO - icrawler.crawler - starting 1 feeder threads...\n",
      "2024-05-30 15:45:00,901 - INFO - feeder - thread feeder-001 exit\n",
      "2024-05-30 15:45:00,902 - INFO - icrawler.crawler - starting 1 parser threads...\n",
      "2024-05-30 15:45:00,904 - INFO - icrawler.crawler - starting 1 downloader threads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded images for Nicolas Cage into face_dataset\\nicolas_cage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-30 15:45:03,225 - INFO - parser - parsing result page https://www.google.com/search?q=random+people&ijn=0&start=0&tbs=&tbm=isch\n",
      "2024-05-30 15:45:05,347 - INFO - parser - no more page urls for thread parser-001 to parse\n",
      "2024-05-30 15:45:05,348 - INFO - parser - thread parser-001 exit\n",
      "2024-05-30 15:45:05,920 - INFO - downloader - no more download task for thread downloader-001\n",
      "2024-05-30 15:45:05,922 - INFO - downloader - thread downloader-001 exit\n",
      "2024-05-30 15:45:06,909 - INFO - icrawler.crawler - Crawling task done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded images for random people into face_dataset\\others\n"
     ]
    }
   ],
   "source": [
    "# Download images for Nicolas Cage and others\n",
    "download_images('Nicolas Cage', 50, os.path.join(base_dir, 'nicolas_cage'))\n",
    "download_images('random people', 50, os.path.join(base_dir, 'others'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
